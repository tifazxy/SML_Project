{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "isInteractiveWindowMessageCell": true
   },
   "source": [
    "Connected to venv (Python 3.8.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.73344103 0.73344103 0.73344103 0.73344103 0.73344103 0.73462783\n",
      " 0.73462783 0.73462783 0.73462783 0.73462783]\n",
      "Mean CV Accuracy: 0.7340344328204484\n",
      "Validation Accuracy: 0.7051282051282052\n",
      "Test Accuracy: 0.7368024132730016\n",
      "Training Label distribution: [0.73403395 0.26596605]\n",
      "Validation Label distribution: [0.70512821 0.29487179]\n",
      "Testing Label distribution: [0.73680241 0.26319759]\n",
      "==================================\n",
      "SVC: tokenized dataset:\n",
      "--- 16.222812175750732 seconds ---\n",
      "Best Hyperparameters: {'C': 20, 'degree': 3, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "Test Accuracy: 0.9969834087481146\n",
      "==================================\n",
      "SVC: PCA processed dataset:\n",
      "--- 0.32158803939819336 seconds ---\n",
      "Best Hyperparameters: {'C': 6, 'degree': 2, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Test Accuracy: 0.9803921568627451\n",
      "==================================\n",
      "Random Forest: tokenized dataset:\n",
      "--- 14.899150133132935 seconds ---\n",
      "Best Hyperparameters: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 161}\n",
      "Test Accuracy: 0.9917043740573153\n",
      "==================================\n",
      "Random Forest: PCA processed dataset:\n",
      "--- 8.791980981826782 seconds ---\n",
      "Best Hyperparameters: {'max_depth': 10, 'min_samples_split': 9, 'n_estimators': 65}\n",
      "Test Accuracy: 0.975867269984917\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# data_v2 tokenized data\n",
    "# data_processed_pca tokenized and also dimension reduced data\n",
    "with open('../data/processed_data/data_v2.pkl', 'rb') as f:\n",
    "    data_reload = pickle.load(f)\n",
    "\n",
    "X_train = data_reload['X_train']\n",
    "X_test  = data_reload['X_test']\n",
    "X_val   = data_reload['X_val']\n",
    "y_train = data_reload['y_train']\n",
    "y_test  = data_reload['y_test']\n",
    "y_val   = data_reload['y_val']\n",
    "\n",
    "with open('../data/processed_data/data_processed_pca.pkl', 'rb') as f:\n",
    "    data_reload_pca = pickle.load(f)\n",
    "\n",
    "X_train_pca = data_reload_pca['X_train']\n",
    "X_test_pca  = data_reload_pca['X_test']\n",
    "X_val_pca   = data_reload_pca['X_val']\n",
    "y_train_pca = data_reload_pca['y_train']\n",
    "y_test_pca  = data_reload_pca['y_test']\n",
    "y_val_pca   = data_reload_pca['y_val']\n",
    "\n",
    "# Define the baseline model(most_frequent class classifier)\n",
    "baseline_model = DummyClassifier(strategy='most_frequent')\n",
    "\n",
    "\n",
    "# try predict\n",
    "def cv_baseline(X_train, y_train):\n",
    "    cv_scores = cross_val_score(baseline_model, X_train, y_train, cv=10, scoring='accuracy')\n",
    "    print(\"Cross-Validation Scores:\", cv_scores)\n",
    "    print(\"Mean CV Accuracy:\", cv_scores.mean())\n",
    "\n",
    "def training_baseline(X_train, y_train):\n",
    "    baseline_model.fit(X_train, y_train)\n",
    "\n",
    "def validation_baseline(X_val, y_val):\n",
    "    y_val_pred = baseline_model.predict(X_val)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    print(\"Validation Accuracy:\", val_accuracy)\n",
    "\n",
    "def prediction_baseline(X_test, y_test):\n",
    "    y_pred = baseline_model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "cv_baseline(X_train, y_train)\n",
    "training_baseline(X_train, y_train)\n",
    "\n",
    "validation_baseline(X_val, y_val)\n",
    "\n",
    "prediction_baseline(X_test, y_test)\n",
    "\n",
    "import numpy as np\n",
    "print(\"Training Label distribution:\", np.bincount(y_train)/ len(y_train))\n",
    "print(\"Validation Label distribution:\", np.bincount(y_val)/ len(y_val))\n",
    "print(\"Testing Label distribution:\", np.bincount(y_test)/ len(y_test))\n",
    "\n",
    "\n",
    "# Another model\n",
    "param_dist_svc = {\n",
    "    'C': randint(1, 100),            # Regularization parameter\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],  # Kernel type\n",
    "    'gamma': ['scale', 'auto'],      # Kernel coefficient\n",
    "    'degree': randint(1, 10),        # Degree of the polynomial kernel\n",
    "}\n",
    "# # try predict\n",
    "# def cv_svc(X_train, y_train):\n",
    "#     cv_scores = cross_val_score(svc, X_train, y_train, cv=10, scoring='accuracy')\n",
    "#     print(\"Cross-Validation Scores:\", cv_scores)\n",
    "#     print(\"Mean CV Accuracy:\", cv_scores.mean())\n",
    "\n",
    "# def training_svc(X_train, y_train):\n",
    "#     svc.fit(X_train, y_train)\n",
    "\n",
    "# def validation_svc(X_val, y_val):\n",
    "#     y_val_pred = svc.predict(X_val)\n",
    "#     val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "#     print(\"Validation Accuracy:\", val_accuracy)\n",
    "\n",
    "# def prediction_svc(X_test, y_test):\n",
    "#     y_pred = svc.predict(X_test)\n",
    "#     test_accuracy = accuracy_score(y_test, y_pred)\n",
    "#     print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "def workflow(model, param_dist, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    start_time = time.time()\n",
    "    random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_jobs=10, n_iter=10, cv=5)\n",
    "    random_search.fit(X_val, y_val)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    # Get the best hyperparameters\n",
    "    best_params = random_search.best_params_\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "    # Train the model\n",
    "    best_model = random_search.best_estimator_\n",
    "    best_model.fit(X_train, y_train)\n",
    "    # Predict\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "print(\"==================================\")\n",
    "print(\"SVC: tokenized dataset:\")\n",
    "workflow(SVC(), param_dist_svc, X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "print(\"==================================\")\n",
    "print(\"SVC: PCA processed dataset:\")\n",
    "workflow(SVC(), param_dist_svc, X_train_pca, y_train_pca, X_val_pca, y_val_pca, X_test_pca, y_test_pca)\n",
    "\n",
    "# Random Forest\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 200),\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': randint(2, 11)\n",
    "}\n",
    "# def training_rf(X_train, y_train):\n",
    "#     rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# def validation_rf(X_val, y_val):\n",
    "#     y_val_pred = rf_classifier.predict(X_val)\n",
    "#     val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "#     print(\"Validation Accuracy:\", val_accuracy)\n",
    "\n",
    "# def prediction_rf(X_test, y_test):\n",
    "#     y_pred = rf_classifier.predict(X_test)\n",
    "#     test_accuracy = accuracy_score(y_test, y_pred)\n",
    "#     print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "print(\"==================================\")\n",
    "print(\"Random Forest: tokenized dataset:\")\n",
    "workflow(RandomForestClassifier(n_estimators=100, random_state=42), param_dist_rf, X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "print(\"==================================\")\n",
    "print(\"Random Forest: PCA processed dataset:\")\n",
    "workflow(RandomForestClassifier(n_estimators=100, random_state=42), param_dist_rf, X_train_pca, y_train_pca, X_val_pca, y_val_pca, X_test_pca, y_test_pca)\n",
    "\n",
    "# Overfitting analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
